{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c06fdcc-421a-46c2-949d-a579b1e20e55",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center\">\n",
    "    <a href=\"https://skills.network\" target=\"_blank\">\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\"  />\n",
    "    </a>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61dae894-7e6f-4049-ad89-28c7295a386e",
   "metadata": {},
   "source": [
    "# **Removing Duplicates**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95fea4c1-f8e5-4ca1-9fd9-5efb239e44c7",
   "metadata": {},
   "source": [
    "Estimated time needed: **45 to 60** minutes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592eb6d9-0ba6-4c28-9adb-e972852b6943",
   "metadata": {},
   "source": [
    "## Introduction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd76f319-8e7e-408a-a2cc-53a78c4d48d4",
   "metadata": {},
   "source": [
    "In this lab, you will focus on data wrangling, an important step in preparing data for analysis. Data wrangling involves cleaning and organizing data to make it suitable for analysis. One key task in this process is removing duplicate entries, which are repeated entries that can distort analysis and lead to inaccurate conclusions.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595c4424-6dcb-491e-aecd-ac830c71a393",
   "metadata": {},
   "source": [
    "## Objectives\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0b98e7-c074-47d7-8fc0-61286f3031aa",
   "metadata": {},
   "source": [
    "In this lab you will perform the following:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8be0a5-d65c-43c0-af2c-fb6e8e9251f3",
   "metadata": {},
   "source": [
    "1. Identify duplicate rows  in the dataset.\n",
    "2. Use suitable techniques to remove duplicate rows and verify the removal.\n",
    "3. Summarize how to handle missing values appropriately.\n",
    "4. Use ConvertedCompYearly to normalize compensation data.\n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ef63f6-5f6f-429a-9776-d3d9acc1a432",
   "metadata": {},
   "source": [
    "### Install the Required Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64a7efff-df6d-45b7-a0e5-b57e2c098c80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\liyana_bh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.3.2)\n",
      "Requirement already satisfied: numpy>=1.23.2 in c:\\users\\liyana_bh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2.3.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\liyana_bh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\liyana_bh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\liyana_bh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\liyana_bh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2694af-c2e4-4755-a900-efc385425f1f",
   "metadata": {},
   "source": [
    "### Step 1: Import Required Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32ff93ab-d691-4619-b164-77f8f4b8afd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a26da2-5e8e-40b5-aa77-e3572a408466",
   "metadata": {},
   "source": [
    "### Step 2: Load the Dataset into a DataFrame\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d89389-bc92-4957-a7e8-3940a651b158",
   "metadata": {},
   "source": [
    "#### **Read Data**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0036976-1f29-4986-a7bc-e025b19b4b84",
   "metadata": {},
   "source": [
    "If you are using JupyterLite, use the code below to download the dataset into your environment. If you are using a local environment, you can use the direct URL with <code>pd.read_csv()</code>.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c59463d7-475e-489d-bddb-93950e5380a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pyodide.http import pyfetch\n",
    "\n",
    "#async def download(url, filename):\n",
    "    #response = await pyfetch(url)\n",
    "    #if response.status == 200:\n",
    "        #with open(filename, \"wb\") as f:\n",
    "            #f.write(await response.bytes())\n",
    "\n",
    "# Define the file path for the data\n",
    "#file_path = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/n01PQ9pSmiRX6520flujwQ/survey-data.csv\"\n",
    "\n",
    "# Download the dataset\n",
    "#await download(file_path, \"survey_data.csv\")\n",
    "#file_name = \"survey_data.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/VYPrOu0Vs3I0hKLLjiPGrA/survey-data-with-duplicate.csv\"\n",
    "df = pd.read_csv(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ResponseId                      MainBranch                 Age  \\\n",
      "0           1  I am a developer by profession  Under 18 years old   \n",
      "1           2  I am a developer by profession     35-44 years old   \n",
      "2           3  I am a developer by profession     45-54 years old   \n",
      "3           4           I am learning to code     18-24 years old   \n",
      "4           5  I am a developer by profession     18-24 years old   \n",
      "\n",
      "            Employment RemoteWork   Check  \\\n",
      "0  Employed, full-time     Remote  Apples   \n",
      "1  Employed, full-time     Remote  Apples   \n",
      "2  Employed, full-time     Remote  Apples   \n",
      "3   Student, full-time        NaN  Apples   \n",
      "4   Student, full-time        NaN  Apples   \n",
      "\n",
      "                                    CodingActivities  \\\n",
      "0                                              Hobby   \n",
      "1  Hobby;Contribute to open-source projects;Other...   \n",
      "2  Hobby;Contribute to open-source projects;Other...   \n",
      "3                                                NaN   \n",
      "4                                                NaN   \n",
      "\n",
      "                                             EdLevel  \\\n",
      "0                          Primary/elementary school   \n",
      "1       Bachelor’s degree (B.A., B.S., B.Eng., etc.)   \n",
      "2    Master’s degree (M.A., M.S., M.Eng., MBA, etc.)   \n",
      "3  Some college/university study without earning ...   \n",
      "4  Secondary school (e.g. American high school, G...   \n",
      "\n",
      "                                           LearnCode  \\\n",
      "0                             Books / Physical media   \n",
      "1  Books / Physical media;Colleague;On the job tr...   \n",
      "2  Books / Physical media;Colleague;On the job tr...   \n",
      "3  Other online resources (e.g., videos, blogs, f...   \n",
      "4  Other online resources (e.g., videos, blogs, f...   \n",
      "\n",
      "                                     LearnCodeOnline  ... JobSatPoints_6  \\\n",
      "0                                                NaN  ...            NaN   \n",
      "1  Technical documentation;Blogs;Books;Written Tu...  ...            0.0   \n",
      "2  Technical documentation;Blogs;Books;Written Tu...  ...            NaN   \n",
      "3  Stack Overflow;How-to videos;Interactive tutorial  ...            NaN   \n",
      "4  Technical documentation;Blogs;Written Tutorial...  ...            NaN   \n",
      "\n",
      "  JobSatPoints_7 JobSatPoints_8 JobSatPoints_9 JobSatPoints_10  \\\n",
      "0            NaN            NaN            NaN             NaN   \n",
      "1            0.0            0.0            0.0             0.0   \n",
      "2            NaN            NaN            NaN             NaN   \n",
      "3            NaN            NaN            NaN             NaN   \n",
      "4            NaN            NaN            NaN             NaN   \n",
      "\n",
      "  JobSatPoints_11           SurveyLength SurveyEase ConvertedCompYearly JobSat  \n",
      "0             NaN                    NaN        NaN                 NaN    NaN  \n",
      "1             0.0                    NaN        NaN                 NaN    NaN  \n",
      "2             NaN  Appropriate in length       Easy                 NaN    NaN  \n",
      "3             NaN               Too long       Easy                 NaN    NaN  \n",
      "4             NaN              Too short       Easy                 NaN    NaN  \n",
      "\n",
      "[5 rows x 114 columns]\n"
     ]
    }
   ],
   "source": [
    "# Display the first few rows\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8fbb262-83ad-4b4b-9a66-60b942ebe17c",
   "metadata": {},
   "source": [
    "**Load the data into a pandas dataframe:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84381dc9-7d78-4399-9733-2845bf533498",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08cefe1d-ebd0-4b75-8c31-41165bd70ee6",
   "metadata": {},
   "source": [
    "**Note: If you are working on a local Jupyter environment, you can use the URL directly in the <code>pandas.read_csv()</code>  function as shown below:**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1055e943-8523-48d6-a9e8-528f712a6fba",
   "metadata": {},
   "source": [
    "##### df = pd.read_csv(\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/n01PQ9pSmiRX6520flujwQ/survey-data.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac2e82f-3e0b-4c5d-94d2-b3a45953d0b3",
   "metadata": {},
   "source": [
    "### Step 3: Identifying Duplicate Rows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c07e51cf-5025-449d-8502-2253e1f5705b",
   "metadata": {},
   "source": [
    "**Task 1: Identify Duplicate Rows**\n",
    "  1. Count the number of duplicate rows in the dataset.\n",
    "  2. Display the first few duplicate rows to understand their structure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "829ae242-52c6-43ff-8aa3-a4b9dde8f8d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate rows: 20\n"
     ]
    }
   ],
   "source": [
    "# your code goes here\n",
    "num_duplicates = df.duplicated().sum()\n",
    "print(\"Number of duplicate rows:\", num_duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First few duplicate rows:\n",
      "       ResponseId                      MainBranch                 Age  \\\n",
      "65437           1  I am a developer by profession  Under 18 years old   \n",
      "65438           2  I am a developer by profession     35-44 years old   \n",
      "65439           3  I am a developer by profession     45-54 years old   \n",
      "65440           4           I am learning to code     18-24 years old   \n",
      "65441           5  I am a developer by profession     18-24 years old   \n",
      "\n",
      "                Employment RemoteWork   Check  \\\n",
      "65437  Employed, full-time     Remote  Apples   \n",
      "65438  Employed, full-time     Remote  Apples   \n",
      "65439  Employed, full-time     Remote  Apples   \n",
      "65440   Student, full-time        NaN  Apples   \n",
      "65441   Student, full-time        NaN  Apples   \n",
      "\n",
      "                                        CodingActivities  \\\n",
      "65437                                              Hobby   \n",
      "65438  Hobby;Contribute to open-source projects;Other...   \n",
      "65439  Hobby;Contribute to open-source projects;Other...   \n",
      "65440                                                NaN   \n",
      "65441                                                NaN   \n",
      "\n",
      "                                                 EdLevel  \\\n",
      "65437                          Primary/elementary school   \n",
      "65438       Bachelor’s degree (B.A., B.S., B.Eng., etc.)   \n",
      "65439    Master’s degree (M.A., M.S., M.Eng., MBA, etc.)   \n",
      "65440  Some college/university study without earning ...   \n",
      "65441  Secondary school (e.g. American high school, G...   \n",
      "\n",
      "                                               LearnCode  \\\n",
      "65437                             Books / Physical media   \n",
      "65438  Books / Physical media;Colleague;On the job tr...   \n",
      "65439  Books / Physical media;Colleague;On the job tr...   \n",
      "65440  Other online resources (e.g., videos, blogs, f...   \n",
      "65441  Other online resources (e.g., videos, blogs, f...   \n",
      "\n",
      "                                         LearnCodeOnline  ... JobSatPoints_6  \\\n",
      "65437                                                NaN  ...            NaN   \n",
      "65438  Technical documentation;Blogs;Books;Written Tu...  ...            0.0   \n",
      "65439  Technical documentation;Blogs;Books;Written Tu...  ...            NaN   \n",
      "65440  Stack Overflow;How-to videos;Interactive tutorial  ...            NaN   \n",
      "65441  Technical documentation;Blogs;Written Tutorial...  ...            NaN   \n",
      "\n",
      "      JobSatPoints_7 JobSatPoints_8 JobSatPoints_9 JobSatPoints_10  \\\n",
      "65437            NaN            NaN            NaN             NaN   \n",
      "65438            0.0            0.0            0.0             0.0   \n",
      "65439            NaN            NaN            NaN             NaN   \n",
      "65440            NaN            NaN            NaN             NaN   \n",
      "65441            NaN            NaN            NaN             NaN   \n",
      "\n",
      "      JobSatPoints_11           SurveyLength SurveyEase ConvertedCompYearly  \\\n",
      "65437             NaN                    NaN        NaN                 NaN   \n",
      "65438             0.0                    NaN        NaN                 NaN   \n",
      "65439             NaN  Appropriate in length       Easy                 NaN   \n",
      "65440             NaN               Too long       Easy                 NaN   \n",
      "65441             NaN              Too short       Easy                 NaN   \n",
      "\n",
      "      JobSat  \n",
      "65437    NaN  \n",
      "65438    NaN  \n",
      "65439    NaN  \n",
      "65440    NaN  \n",
      "65441    NaN  \n",
      "\n",
      "[5 rows x 114 columns]\n"
     ]
    }
   ],
   "source": [
    "# Display the first few duplicate rows\n",
    "duplicate_rows = df[df.duplicated()]\n",
    "print(\"\\nFirst few duplicate rows:\")\n",
    "print(duplicate_rows.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eabe8f9-8d48-406e-9364-9385cc2e93dd",
   "metadata": {},
   "source": [
    "### Step 4: Removing Duplicate Rows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a612ce2-e214-468d-863c-319889bd7f03",
   "metadata": {},
   "source": [
    "**Task 2: Remove Duplicates**\n",
    "   1. Remove duplicate rows from the dataset using the drop_duplicates() function.\n",
    "2. Verify the removal by counting the number of duplicate rows after removal .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c7baa7d9-53e8-4237-bfcd-e3228c92aae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here\n",
    "# 1. Remove duplicate rows\n",
    "df_cleaned = df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate rows after removal: 0\n"
     ]
    }
   ],
   "source": [
    "# 2. Verify removal by counting duplicates again\n",
    "num_duplicates_after = df_cleaned.duplicated().sum()\n",
    "\n",
    "print(\"Number of duplicate rows after removal:\", num_duplicates_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset shape: (65457, 114)\n",
      "Cleaned dataset shape: (65437, 114)\n"
     ]
    }
   ],
   "source": [
    "# (Optional) check shape before and after\n",
    "print(\"Original dataset shape:\", df.shape)\n",
    "print(\"Cleaned dataset shape:\", df_cleaned.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be06f8d7-a07a-489e-a5bd-c4b84e8c5f7c",
   "metadata": {},
   "source": [
    "### Step 5: Handling Missing Values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9ad921-45a0-492c-b52e-062204402a25",
   "metadata": {},
   "source": [
    "**Task 3: Identify and Handle Missing Values**\n",
    "   1. Identify missing values for all columns in the dataset.\n",
    "   2. Choose a column with significant missing values (e.g., EdLevel) and impute with the most frequent value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ae28feab-86b7-4202-bb6f-e92c926eb99d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            missing_count  missing_%\n",
      "AINextMuch less integrated          64309      98.25\n",
      "AINextLess integrated               63102      96.40\n",
      "AINextNo change                     52955      80.90\n",
      "AINextMuch more integrated          52018      79.47\n",
      "EmbeddedAdmired                     48718      74.43\n",
      "...                                   ...        ...\n",
      "MainBranch                              0       0.00\n",
      "Check                                   0       0.00\n",
      "Employment                              0       0.00\n",
      "Age                                     0       0.00\n",
      "ResponseId                              0       0.00\n",
      "\n",
      "[114 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# your code goes here\n",
    "\n",
    "# 1) Missing values for all columns (count and %)\n",
    "na_count = df.isna().sum().sort_values(ascending=False)\n",
    "na_pct   = (df.isna().mean() * 100).round(2)\n",
    "missing_summary = pd.concat([na_count, na_pct], axis=1)\n",
    "missing_summary.columns = [\"missing_count\", \"missing_%\"]\n",
    "print(missing_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Imputing column: EdLevel\n",
      "Most frequent value (mode): Bachelor’s degree (B.A., B.S., B.Eng., etc.)\n"
     ]
    }
   ],
   "source": [
    "# 2) Impute a column with many missings (e.g., 'EdLevel') with its most frequent value (mode)\n",
    "col = \"EdLevel\" if \"EdLevel\" in df.columns else na_count.index[0]  # fallback to most-missing column\n",
    "mode_val = df[col].mode(dropna=True)[0] if not df[col].mode(dropna=True).empty else None\n",
    "\n",
    "print(f\"\\nImputing column: {col}\")\n",
    "print(\"Most frequent value (mode):\", mode_val)\n",
    "\n",
    "df[col] = df[col].fillna(mode_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing after imputation:\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# Verify imputation\n",
    "print(\"\\nMissing after imputation:\")\n",
    "print(df[col].isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86335c8e-529d-4a63-82f6-96e222f07a1e",
   "metadata": {},
   "source": [
    "### Step 6: Normalizing Compensation Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404a5e09-d680-49b6-ae78-6e41274d56dc",
   "metadata": {},
   "source": [
    "**Task 4: Normalize Compensation Data Using ConvertedCompYearly**\n",
    "   1. Use the ConvertedCompYearly column for compensation analysis as the normalized annual compensation is already provided.\n",
    "   2. Check for missing values in ConvertedCompYearly and handle them if necessary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dfafd49c-d4c5-4fc4-8266-ecad29e6480d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in ConvertedCompYearly: 42020\n",
      "\n",
      "After imputation:\n",
      "Missing values in ConvertedCompYearly: 0\n",
      "Median annual compensation used for imputation: 65000.0\n"
     ]
    }
   ],
   "source": [
    "# your code goes here\n",
    "# 1) Check the column exists\n",
    "if \"ConvertedCompYearly\" in df.columns:\n",
    "    # 2) Check missing values\n",
    "    missing_comp = df[\"ConvertedCompYearly\"].isna().sum()\n",
    "    print(\"Missing values in ConvertedCompYearly:\", missing_comp)\n",
    "    \n",
    "    # 3) Handle missing values (impute with median for compensation data)\n",
    "    median_comp = df[\"ConvertedCompYearly\"].median()\n",
    "    df[\"ConvertedCompYearly\"] = df[\"ConvertedCompYearly\"].fillna(median_comp)\n",
    "    \n",
    "    print(\"\\nAfter imputation:\")\n",
    "    print(\"Missing values in ConvertedCompYearly:\", df[\"ConvertedCompYearly\"].isna().sum())\n",
    "    print(\"Median annual compensation used for imputation:\", median_comp)\n",
    "else:\n",
    "    print(\"ConvertedCompYearly column not found in dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ec16e0-6dc9-4405-851a-7b8806d77775",
   "metadata": {},
   "source": [
    "### Step 7: Summary and Next Steps\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4939509b-0488-40b9-8f8b-383017ea6242",
   "metadata": {},
   "source": [
    "**In this lab, you focused on identifying and removing duplicate rows.**\n",
    "\n",
    "- You handled missing values by imputing the most frequent value in a chosen column.\n",
    "\n",
    "- You used ConvertedCompYearly for compensation normalization and handled missing values.\n",
    "\n",
    "- For further analysis, consider exploring other columns or visualizing the cleaned dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f4bad0-a216-4721-b6e7-c0fe1cd51e18",
   "metadata": {},
   "source": [
    "<!--\n",
    "## Change Log\n",
    "\n",
    "|Date (YYYY-MM-DD)|Version|Changed By|Change Description|\n",
    "|-|-|-|-|\n",
    "|2024-11-05|1.2|Madhusudhan Moole|Updated lab|\n",
    "|2024-09-24|1.1|Madhusudhan Moole|Updated lab|\n",
    "|2024-09-23|1.0|Raghul Ramesh|Created lab|\n",
    "\n",
    "--!>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6810f006-53ee-493e-a29f-837f4ab73310",
   "metadata": {},
   "source": [
    "## <h3 align=\"center\"> © IBM Corporation. All rights reserved. <h3/>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "prev_pub_hash": "f78dd679c054b4b4ba83764fae76e380612ff2780b7f9452119e6c3f45824b5a"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
